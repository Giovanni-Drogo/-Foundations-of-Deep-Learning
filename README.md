# -Foundations-of-Deep-Learning
The content of this experiment involves modifying average pooling to max pooling, changing the activation function from Sigmoid to ReLU, adjusting the number of fully connected layers, modifying the learning rate and training epochs, and removing the pooling layer before the fully connected layers for handwritten digit recognition. 

To compare the performance of the models after different modifications, I recorded and calculated the accuracy, precision, recall, and F1-score on the test set, and generated a confusion matrix. 

These metrics can help us gain a comprehensive understanding of the model's performance across different classes, and identify potential issues such as class imbalance or overfitting.

Even more exciting is that I had the model recognize my handwritten digits 3 or 9, yet it consistently predicted them as 1.