# -Foundations-of-Deep-Learning
This experiment is based on the content of Lecture 8 (Natural Language Processing) and Lecture 9 (Transformer). In this experiment, I utilized Ollama to deploy a small large language model locally. Given that my computer has a CPU and no GPU, I deployed the lightweight model `smallthinker:latest` and completed its invocation using various methods. Furthermore, I explored the existing translation model `Helsinki-NLP` on Hugging Face to accomplish mutual translations between various languages.